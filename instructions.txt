

Please use the CheXzero repo or OpenCLIP to train a CLIP model on CheXpert-Plus plus ReXGradient. Before you start, please briefly skim the CheXpert overview page (https://urldefense.com/v3/__https://stanfordmlgroup.github.io/competitions/chexpert/__;!!IBzWLUs!QVI5tOU1ljJ5SRiLbsyJfhEfep3VKQBbWJNbs_6fG1m4G9CljaIoPSDoU599VUys_FSqPt5nw-Ig94x_8V0f7owo4cnI8pgSG0Dt$ ) and the CheXpert-Plus (https://urldefense.com/v3/__https://arxiv.org/abs/2405.19538__;!!IBzWLUs!QVI5tOU1ljJ5SRiLbsyJfhEfep3VKQBbWJNbs_6fG1m4G9CljaIoPSDoU599VUys_FSqPt5nw-Ig94x_8V0f7owo4cnI8vKPHp9Z$ ) and ReXGradient (https://urldefense.com/v3/__https://arxiv.org/abs/2505.00228__;!!IBzWLUs!QVI5tOU1ljJ5SRiLbsyJfhEfep3VKQBbWJNbs_6fG1m4G9CljaIoPSDoU599VUys_FSqPt5nw-Ig94x_8V0f7owo4cnI8t-t8ENW$ ) papers so you are familiar with the dataset conventions and splits.

To do this, you will need access to the project folder at /cbica/projects/CXR, and both datasets should be under /cbica/projects/CXR/data/. Prior to training, please construct the datasets exactly as follows: use the CheXpert-Plus training split together with the full ReXGradient dataset as the training set, use CheXpert-Plus’s validation split as our validation set, and use CheXpert-Plus’s test split as our final test set. It is critical that there is no patient overlap among train, validation, and test, including any potential overlap introduced through ReXGradient, so please explicitly verify and document that.

Once the dataset is ready, please apply the same preprocessing used in CheXzero as implemented in their repository (https://urldefense.com/v3/__https://github.com/rajpurkarlab/CheXzero__;!!IBzWLUs!QVI5tOU1ljJ5SRiLbsyJfhEfep3VKQBbWJNbs_6fG1m4G9CljaIoPSDoU599VUys_FSqPt5nw-Ig94x_8V0f7owo4cnI8lMBqnzv$ ). For the initial model configuration, use DINOv3 ViT-B as the vision encoder and use CheXzero’s text encoder. During training, you will likely notice the model begins to overfit after only a few epochs, so please follow the PLIP strategy of validating very frequently and saving checkpoints by training steps rather than by epochs (https://urldefense.com/v3/__https://www.nature.com/articles/s41591-023-02504-3*Sec10__;Iw!!IBzWLUs!QVI5tOU1ljJ5SRiLbsyJfhEfep3VKQBbWJNbs_6fG1m4G9CljaIoPSDoU599VUys_FSqPt5nw-Ig94x_8V0f7owo4cnI8hQcSlnf$ ).

When training is complete, please report the zero-shot AUROC on the CheXpert validation and test sets (14 CXR labels) and compare the results to the performance reported in the CheXzero paper. Please also include the training loss curve and the validation AUROC trend over steps, and clearly state which checkpoint you selected and why.
